import argparse
import re
from pathlib import Path
from bs4 import BeautifulSoup
import sys
import ollama


def remove_non_arabic(text):
    # Arabic character range in Unicode is \u0600 to \u06FF
    # This regular expression will match only Arabic characters
    # and spaces.
    arabic_text = re.sub(r'[^\u0600-\u06FF\s]', '', text)
    # Replace multiple whitespace with a single space
    arabic_text = re.sub(r'\s+', ' ', arabic_text)
    return arabic_text


def split_arabic_sentences(text):
    # Regular expression to match each sentence ending with Arabic punctuation
    # Sentence endings: Arabic period (۔), Arabic question mark (؟), Arabic exclamation mark (!)
    sentence_pattern = r'.+?[۔؟!]+|.+$'

    # Use re.findall to find all matches for the pattern
    sentences = re.findall(sentence_pattern, text)

    # Strip any surrounding whitespace from each sentence and return the list
    return [sentence.strip() for sentence in sentences]


def clean_arabic(text):
    # This is a stub generated by ChatGPT, there is no guarantee it will work as expected.
    # Please verify and modify it if needed.

    # Remove diacritics (Arabic vowel marks)
    text = re.sub(r'[\u0610-\u061A\u064B-\u065F\u0670\u06D6-\u06DC\u06DF-\u06E8\u06EA-\u06ED]', '', text)

    # Remove punctuation and non-essential symbols
    text = re.sub(r'[^\w\u0600-\u06FF]', '', text)

    # Remove whitespace
    text = re.sub(r'\s+', '', text)

    return text


def check_if_identical(text1, text2):
    # Compare the cleaned texts
    return clean_arabic(text1) == clean_arabic(text2)


def segment_paragraph(para: BeautifulSoup, para_counter: int, s_counter: int) -> list:
    '''
    Segment a paragraph into sentences.
    '''
    # add id attribute to para element of format p001 etc.
    para['id'] = f'p{para_counter:03}'

    prompt = ("The following is a very long Arabic sentence, which is bad style and hard to read. "
              "Break it up into shorter sentences by inserting a period where you think one should go. "
              "Do not change anything else in the sentence apart from adding periods:\n")

    para_text = para.text
    para_text = re.sub(r'\s+', ' ', para_text)

    iteration = 1
    while True:
        response = ollama.generate(prompt=prompt + para_text, model="gemma2")

        # the point of this is to remove stuff like
        # "I will break up the very long sentence into shorter sentences for you"
        # from the continuation generated by the model
        segmented_output = remove_non_arabic(response['response'])
        print(f"Iteration {iteration}:")
        print(f"Segmented output: {segmented_output}")
        sentences = split_arabic_sentences(segmented_output)
        print(f"Number of sentences: {len(sentences)}")
        for sentence in sentences:
            print(sentence)
        if check_if_identical(para_text, segmented_output):
            break
        else:
            print()
            iteration += 1

    # split the segmented_output into sentences using the Arabic period as the delimiter
    sentences = split_arabic_sentences(segmented_output)

    # delete all content from para element
    para.clear()
    for sentence in sentences:
        # create s element using BeautifulSoup with id attribute of format s001 etc.
        s = BeautifulSoup(f'<s id="s{s_counter:03}">{sentence}</s>', 'xml')
        # insert a line break after each sentence element
        s.append("\n")
        para.append(s)
        s_counter += 1
    return para, para_counter + 1, s_counter


def main():
    args = parse_args()
    with open(args.input_file, encoding='utf-8') as xml_file:
        soup = BeautifulSoup(xml_file, "xml")

    outfile = open("Albucasis_Chirurgia_ar_sentences", "w", encoding='utf-8')

    # Delete all note, lb and pb elements from soup.
    for note in soup.find_all('note'):
        note.decompose()
    for lb in soup.find_all('lb'):
        lb.decompose()
    for pb in soup.find_all('pb'):
        pb.decompose()

    para_counter = 1
    s_counter = 1

    preface = soup.find('div', {'type': 'preface'})
    for para in preface.find_all('p'):
        segmented_para, next_para, next_s =\
            segment_paragraph(para, para_counter, s_counter)

        # replace original paragraph with segmented paragraph
        para.replace_with(segmented_para)

        # write segmented paragraph to file
        print(soup, file=outfile)
        print(f"{next_para=} {next_s=}")
        sys.exit()

    for sentence in preface.find_all('s'):
        if args.print_s_tags:
            # print sentence element in its entirety
            print(sentence, file=outfile)
        else:
            print(sentence.text, file=outfile)

    for chapter_number in range(1, 4):
        chapter = soup.find('div', {'type': 'chapter', 'n': chapter_number})
        p_children = chapter.find_all('p', recursive=False)

        for p in p_children:
            for sentence in p.find_all('s'):
                if args.print_s_tags:
                    print(sentence, file=outfile)
                else:
                    print(sentence.text, file=outfile)

        subchapters = chapter.find_all('div', {'type': 'subchapter'})

        for subchapter in subchapters:
            subchapter_number = subchapter['n']

            for sentence in subchapter.find_all('s'):
                if args.print_s_tags:
                    print(sentence, file=outfile)
                else:
                    print(sentence.text, file=outfile)

    outfile.close()


def parse_args():
    parser = argparse.ArgumentParser(description='Segment paragraphs into sentences')
    parser.add_argument('input_file', type=str,
                        help='Path to input xml file')
    parser.add_argument('-t', '--print_s_tags', action='store_true',
                        help='Print the s tags')

    return parser.parse_args()


if __name__ == '__main__':
    main()
