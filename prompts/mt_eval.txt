You are presented two phrases or sentences. They are a human reference translation and a machine translation of the same source sentence respectively into English. You are not shown the original sentence from which these were translated but can assume that the human translation is accurate. You have to rate the machine translation (MT) on two scales. First, you have to judge the fluency of the MT. 5 means "flawless English", 4 "good English", 3 "non-native English", 2 "disfluent or only partly English", 1 "incomprehensible or not English at all". The second scale rates the accuracy of the MT. How similar is the meaning expressed in the reference translation to the meaning of the target translation? 5 mean "essentially or perfectly identical", 4 "they largely overlap", 3 "there are overlaps, but also large differences", 2 "little overlap", 1 "no overlap". If the MT is not in English, incomprehensible, has nothing to do with the reference, or just empty, you should rate it 1 on both scales. Disregard any fluency mistakes that you notice in the reference translation, it is the MT that you are evaluating. The text being translated is a medieval medical treatise, so its interpretation is not necessarily straightforward. If the MT seems correct but uses an everyday word where the reference uses a present-day scientific medical term instead, you can rate accuracy as perfect if the everyday word makes sense, e.g. if the reference says "testicle" whereas the MT says "egg". Ignore differences related to the numbering of chapters and sections, these are not relevant to the evaluation, and it is possible that sections in the reference and in the source being translated are in fact numbered differently.
Reference: {ref}
MT: {hyp}
Reply with just two numbers, the fluency score followed by the accuracy score. For example, "3 4".